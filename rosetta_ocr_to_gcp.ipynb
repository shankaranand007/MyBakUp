{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0.dev20181018'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "torch.backends.cudnn.deterministic = True\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "#import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms,datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For csv type of loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOvenmber 23\n",
    "import string\n",
    "char_set_aadhar = string.ascii_letters + ' -/:' + string.digits    \n",
    "map_chars_aadhar = {x:i+1 for i,x in enumerate(set(char_set_aadhar))}\n",
    "map_chars_aadhar['_'] = 0\n",
    "reverse_map_aadhar = {map_chars_aadhar[y]:y for y in map_chars_aadhar.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "class images_random_text_dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# ON the assumption that we give a csv file which has 3 features path,name,wordlength respectively so that they could be acc\n",
    "#        accessed directly through a dataframe and passed to __getitem__ function in dataset\n",
    "        \n",
    "    def __init__(self, dataset_filtered,map_chars,reverse_map,width_=128):\n",
    "        \"\"\" Intialize the dataset\n",
    "        \"\"\"\n",
    "        self.dataset = dataset_filtered\n",
    "        #self.filenames = list(self.dataset.path)\n",
    "        #self.root = root_\n",
    "        self.imgWidth = width_\n",
    "        self.map_chars = map_chars\n",
    "        self.reverse_map = reverse_map\n",
    "        number_of_files = len(self.dataset)\n",
    "        self.root_dir = root_dir\n",
    "        self.number_of_files = len(self.dataset)\n",
    "\n",
    "        \n",
    "    # You must override __getitem__ and __len__\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        ## CSV file Column Names\n",
    "        dataset = self.dataset\n",
    "        #print(dataset.head())\n",
    "        image_path_list = list(dataset.path)\n",
    "        #print(len(image_path_list))\n",
    "        #print('image_path_list type :'+ str(type(image_path_list)))\n",
    "        data_wordlength = list(dataset.wordlength)\n",
    "        data_name_list = list(dataset.name)\n",
    "        ##############\n",
    "        #relative_path = image_path_list[index]\n",
    "        #full_path = \n",
    "        ################\n",
    "        #print(index)\n",
    "        image = Image.open(image_path_list[index])\n",
    "        \n",
    "        orig_name_,name_length = data_name_list[index],data_wordlength[index]\n",
    "        ## USED LOWER BECAUSE THE DICTIONARY IS BUILT ONLY FOR LOWER CASE\n",
    "        name_ = orig_name_.lower() \n",
    "\n",
    "        y_name,y_name_length = self.get_names_as_number_arrays(name_)\n",
    "        if image.size[0] >self.imgWidth:\n",
    "            size_reduce = transforms.Resize((32,self.imgWidth))\n",
    "            image = size_reduce(image)\n",
    "        else:\n",
    "            height = image.size[1]\n",
    "            width = image.size[0]\n",
    "            size_pad = transforms.Pad((0,0,self.imgWidth-width,32-height), fill=(255,255,255), padding_mode='constant')\n",
    "            image = size_pad(image)\n",
    "        self.transform = transforms.ToTensor()\n",
    "        input_image = self.transform(image)\n",
    "\n",
    "        return (input_image,y_name,y_name_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.number_of_files\n",
    "    def get_names_as_number_arrays(self,name_from_df):\n",
    "        targe1 = []\n",
    "        tar_length = []\n",
    "        max_len = 0\n",
    "        name_from_df = str(name_from_df)\n",
    "        count_ = len(name_from_df)\n",
    "        #print(image_path)\n",
    "        #for path in image_paths:\n",
    "        name_ = name_from_df\n",
    "        #print(name_)\n",
    "        name_encoded = self.alphabet_encode(name_)     ## ENcode the name to list of numbers 'a-z' & ' '\n",
    "        #targe1.extend(name_encoded)    ##EXTEND IS USED WITH CUDA\n",
    "        #targe.append(name_)           ## CAN BE USED ONLY WITH \"CPU\"\n",
    "        name_len = len(name_encoded)\n",
    "\n",
    "        return (name_encoded,name_len)\n",
    "\n",
    "    def alphabet_encode(self,text):\n",
    "\n",
    "    \n",
    "        nums = [self.map_chars.get(x,0) for x in text]\n",
    "\n",
    "        return nums\n",
    "    def alphabet_decode(self,num_list):\n",
    "       \n",
    "        name = [self.reverse_map.get(x) for x in num_list]\n",
    "\n",
    "        return ''.join(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_with_wordlength_less_than(df,word_len):\n",
    "    \n",
    "    word_length_less_than = df.wordlength < word_len\n",
    "    dataset_filtered_l = df[word_length_less_than]\n",
    "    #dataset_path_filtered = absolute_to_relative_path(dd_filtered,directory_path)\n",
    "    \n",
    "    return dataset_filtered_l\n",
    "def get_images_with_image_width_less_than(df,img_width):\n",
    "    \n",
    "    image_less_than = df.image_width < img_width\n",
    "    dataset_filtered_w = df[image_less_than]\n",
    "    #dataset_path_filtered = absolute_to_relative_path(dd_filtered,directory_path)\n",
    "    \n",
    "    return dataset_filtered_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input to Data_set_creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "torch.backends.cudnn.deterministic = True\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "#import Levenshtein\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms,datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size,stride):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, kernel_size=3, out_channels=out_channels, stride=1, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "def conv7x7(in_channels, out_channels, stride=2):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=stride, padding=0, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        print('bn1 type' + str(type(self.bn1)))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        #self.part1 = Unit(in_channels=in_channels,kernel_size=3,out_channels=out_channels,stride=stride)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv = conv7x7(3, 64)\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "        self.layer1 = self.make_layer(block, 64, 1)\n",
    "        self.layer2 = self.make_layer(block, 128, 1)\n",
    "        self.layer3 = self.make_layer(block, 256, 1)\n",
    "        self.layer4 = self.make_layer(block, 512, 1)\n",
    "        self.lastConvlayer = conv3x3(512,64)\n",
    "        self.linearLayer = nn.Linear(384,40)\n",
    "        self.probLayer = nn.LogSoftmax(2) #dimension along which log_softmax to be calculated.\n",
    "        #sequence the columns in all channels into a sngle column so you have (c*h,w) dmensional matrix\n",
    "        \n",
    "        \n",
    "        #class_matrix = self.linearLayer(seq_matrix)\n",
    "       \n",
    "        #prob_matrix = self.probLayer(class_matrix)\n",
    "\n",
    "        #self.avg_pool = nn.AvgPool2d(8)\n",
    "        #self.fc = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         print('Input Size ' + str(x.size()))\n",
    "        out = self.conv(x)\n",
    "#         print('After Conv layer ' + str(out.size()))\n",
    "        out = self.bn(out)\n",
    "#         print('After Batchnorm layer ' + str(out.size()))\n",
    "        out = self.relu(out)\n",
    "#         print('After relu layer ' + str(out.size()))\n",
    "        out = self.max_pool(out)\n",
    "#         print('After max_pool layer ' + str(out.size()))\n",
    "        out = self.layer1(out)\n",
    "#         print('After 1st res block layer ' + str(out.size()))\n",
    "        out = self.layer2(out)\n",
    "#         print('After 2nd res block layer ' + str(out.size()))\n",
    "        out = self.layer3(out)\n",
    "#         print('After 3rd res block layer ' + str(out.size()))\n",
    "        out = self.layer4(out)\n",
    "#         print('After 4th res block layer ' + str(out.size()))\n",
    "        out = self.lastConvlayer(out)\n",
    "#         print('After last Conv layer ' + str(out.size()))\n",
    "        t,channels,rows,cols = out.size()\n",
    "        out = out.view(t,channels*rows,cols).transpose(1,2) \n",
    "        print('After reshaping for linear layer to collapse : ' +str(out.size()) )\n",
    "        out = self.linearLayer(out)\n",
    "        print('Linear layer output size :'+ str(out.size()))\n",
    "       \n",
    "        prob_matrix = self.probLayer(out)\n",
    "        print('After last layer ' + str(prob_matrix.size()))\n",
    "        #dim_mod_for_CTC = prob_matrix.permute(1,0,2)\n",
    "        #print('input to CTC Loss function'+ str(dim_mod_for_CTC.size()))\n",
    "        #out = self.avg_pool(out)\n",
    "        #out = out.view(out.size(0), -1)\n",
    "        #out = self.fc(out)\n",
    "        return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabet_encode(text):\n",
    "#     nums = [0]\n",
    "#     for x in text.lower():\n",
    "#         if (x >= 'a' and x <= 'z') or x == ' ':\n",
    "#             nums.append((ord(x) - 96))\n",
    "#             nums.append(0)\n",
    "\n",
    "    nums = [map_chars.get(x,0) for x in text]\n",
    "    #for i in nums\n",
    "    #print(nums)\n",
    "    return nums\n",
    "def encoded_series_to_list(target_encode_series,target_length_list):\n",
    "    pos_list = [sum(target_length_list[0:i]) for i in range(0,len(target_length_list)+1)]\n",
    "    decoded_name = []\n",
    "    for i in range(len(pos_list)-1):\n",
    "        #encoded_name.append(list(vv[1][i:i+]))\n",
    "        #encoded_name.append(list(target_encode_series[pos_list[i]:pos_list[i+1]]))\n",
    "        encoded_name = list(target_encode_series[pos_list[i]:pos_list[i+1]])\n",
    "        decoded_name.append(alphabet_decode_single(encoded_name))\n",
    "    return decoded_name\n",
    "def alphabet_decode_single(num_list):\n",
    "\n",
    "    name = [reverse_map.get(x) for x in num_list]\n",
    "\n",
    "\n",
    "    #name = [chr(int(nu + 96)) if 0<nu<27 else (nu not 0) str(nu-26) for nu in num_list]\n",
    "    return ''.join(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_loss = nn.CTCLoss()\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx,(inputs, target_name_encoded, target_lengths) in enumerate(train_loader):\n",
    "        #print('labels')\n",
    "        #print(labels)\n",
    "        #print('shape')\n",
    "        #print(inputs.shape)\n",
    "        \n",
    "        #target = get_names_as_list()\n",
    "        input_batch_size_ = len(inputs)\n",
    "        image_batch =torch.stack(inputs) \n",
    "        #encoded_target_train, target_lengths = get_names_as_number_arrays(images_paths)\n",
    "        #print('encoded_target_train')\n",
    "        #print(encoded_target_train)\n",
    "        #print('target_lengths')\n",
    "        #print(target_lengths)\n",
    "        #print(paths)\n",
    "        ##print('encoded target : ')\n",
    "        #print(encoded_target_train)\n",
    "        #print(encoded_target_train)\n",
    "        encoded_target_train = torch.LongTensor(target_name_encoded)\n",
    "        #encoded_target_train = torch.LongTensor(encoded_target_train)\n",
    "        optimizer.zero_grad()\n",
    "        #print(encoded_target_train)\n",
    "        image_batch = image_batch.to(device)\n",
    "        print(image_batch.shape)\n",
    "        prob_sequence_matrix = model(image_batch)\n",
    "        #prob_sequence_matrix.\n",
    "        prob_sequence_matrix = prob_sequence_matrix.permute(1,0,2)\n",
    "        ###Normally for a 128 width image input length is it is 30 for the model\n",
    "        in_length_tensor = np.full((1,int(input_batch_size_)),prob_sequence_matrix.shape[0])\n",
    "        in_length_tensor = torch.LongTensor(in_length_tensor)\n",
    "        target_length_tensor = torch.LongTensor(target_lengths)\n",
    "        #loss = F.nll_loss(output, target)\n",
    "        in_length_tensor = in_length_tensor.to(device)\n",
    "        target_length_tensor = target_length_tensor.to(device)\n",
    "        encoded_target_train = encoded_target_train.to(device)\n",
    "#         if batch_idx % 5 == 0:\n",
    "#             for param in model.parameters():\n",
    "#                 print('Param : ' + str(param))\n",
    "#                 print('Param Grad :' + str(param.grad))\n",
    "        #print('input to CTC Loss function'+ str(dim_mod_for_CTC.size()))\n",
    "#         num_mat = []\n",
    "#         for images in range(prob_sequence_matrix.shape[0]):\n",
    "#             numli = []\n",
    "#             for cols in range(prob_sequence_matrix[image].shape[0]):\n",
    "#                 numli.append(np.argmax(prob_sequence_matrix[images][cols].detach().numpy()))\n",
    "#             num_mat.append(numli)\n",
    "        #print('prob_sequence_matrix')\n",
    "        #print(prob_sequence_matrix)\n",
    "#         print('prob_sequence_matrix shape :' + str(prob_sequence_matrix.shape))\n",
    "        #print('encoded_target_train')\n",
    "        #print(encoded_target_train)\n",
    "#         print('encoded_target_train shape :' + str(encoded_target_train.shape))\n",
    "#        print('in_length_tensor' + str(in_length_tensor.shape))\n",
    "#        print('target_length_tensor'+ str(target_length_tensor.shape))\n",
    "        \n",
    "        loss = ctc_loss(prob_sequence_matrix, encoded_target_train, in_length_tensor, target_length_tensor)\n",
    "        writer.add_scalar('train_loss', loss, epoch)  ## To Display in TensorboardX\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(),1)\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(),1)\n",
    "        optimizer.step()\n",
    "        #if batch_idx % args.log_interval == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(inputs), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python\n",
      "\u001b[31m  Could not find a version that satisfies the requirement python (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for python\u001b[0m\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_dist_calc_list(pred_word_list,targ_word_list):\n",
    "    dist = 0\n",
    "    test_batch_size = len(pred_word_list)\n",
    "    #max_len = len(pred_word_list) if len(pred_word_list)>len(targ_word_list) else len(targ_word_list)\n",
    "    for i in range(test_batch_size):\n",
    "        dist += Levenshtein.distance(pred_word_list[i],targ_word_list[i])\n",
    "    mean_dist = dist/test_batch_size\n",
    "    return mean_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabet_decode(num_list):\n",
    "    name = [reverse_map.get(x) for x in num_list]\n",
    "\n",
    "\n",
    "    #name = [chr(int(nu + 96)) if 0<nu<27 else (nu not 0) str(nu-26) for nu in num_list]\n",
    "    return ''.join(name)\n",
    "def collapse(names_encoded):\n",
    "    collapsed_words=[]\n",
    "    for encoded in names_encoded:\n",
    "        for i,n in enumerate(encoded):\n",
    "            if encoded[i] == encoded[i-1]:\n",
    "                encoded[i-1] = 0\n",
    "        collapsed_words.append([x for x in encoded if x!=0 ])\n",
    "    return collapsed_words\n",
    "def inference(model,image_path):\n",
    "    \n",
    "    img = Image.open(image_path)\n",
    "    ch = preprocess(img)\n",
    "    #width = img.size[0]\n",
    "    #height = img.size[1]\n",
    "    ch = ch.view(1,3,32,128)\n",
    "    trial_img = ch.to('cuda')\n",
    "    log_img = model(trial_img)\n",
    "    print(log_img.shape)\n",
    "    cpu_img = log_img.to(device = 'cpu')\n",
    "    final_prob = np.exp(cpu_img.detach().numpy())\n",
    "    encode_char = np.argmax(final_prob,axis = 2)\n",
    "    print(encode_char)\n",
    "    print(encode_char.shape)\n",
    "    #flattened = encode_char.reshape(encode_char.shape[1])\n",
    "    #print(flattened)\n",
    "    #print(flattened.shape)\n",
    "    collapsed = collapse(encode_char)\n",
    "    print(collapsed)\n",
    "    #print(type(collapsed))\n",
    "    for x in collapsed:\n",
    "        word = alphabet_decode(x)\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse(names_encoded):\n",
    "    print(names_encoded)\n",
    "    collapsed_words=[]\n",
    "    for encoded in names_encoded:\n",
    "        for i,n in enumerate(encoded):\n",
    "            if encoded[i] == encoded[i-1]:\n",
    "                encoded[i-1] = 0\n",
    "        collapsed_words.append([x for x in encoded if x!=0 ])\n",
    "    if(len(collapsed_words) == 0):\n",
    "        collapsed_words.append(0)\n",
    "    return collapsed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for nu,(inputs, target_name_encoded, target_lengths) in enumerate(test_loader):\n",
    "            print(target_name_encoded)\n",
    "            #print(type(target_name_encoded))\n",
    "            print(target_lengths)\n",
    "            #data, target = data.to(device), target.to(device)\n",
    "            #images_paths = paths\n",
    "            input_batch_size_ = len(inputs)\n",
    "            image_batch =torch.stack(inputs) \n",
    "            image_batch = image_batch.to(device)\n",
    "            pred_mat = model(image_batch)\n",
    "            #print(pred_mat.shape)\n",
    "            #print('pred mat' + str(pred_mat.size()))\n",
    "            encoded_target_test, target_lengths = target_name_encoded,target_lengths\n",
    "            encoded_target_test = torch.LongTensor(encoded_target_test)\n",
    "            #test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            #pred = np.argmax(pred_mat.detach().numpy(),axis = 2)\n",
    "            \n",
    "#             print('pred')\n",
    "#             print(pred)\n",
    "#             print('encoded_target_test')\n",
    "#             print(encoded_target_test)\n",
    "            cpu_img = pred_mat.to(device = device)\n",
    "            final_prob = np.exp(cpu_img.detach().numpy())\n",
    "            pred_encode_char = np.argmax(final_prob,axis = 2)\n",
    "            #print(encode_char)\n",
    "            #print(encode_char.shape)\n",
    "            #flattened = encode_char.reshape(encode_char.shape[1])\n",
    "            #print(flattened)\n",
    "            #print(flattened.shape)\n",
    "            pred_collapsed = collapse(pred_encode_char)\n",
    "            #print(pred_collapsed)\n",
    "            #print(collapsed)\n",
    "            #pred_words = alphabet_decode_list(pred_collapsed)\n",
    "            #targ_words = encoded_series_to_list(target_name_encoded,target_lengths)\n",
    "            print(pred_collapsed)\n",
    "            #print(targ_words)\n",
    "            #print(pred_words)\n",
    "#             pred_mat = pred_mat.permute(1,0,2)\n",
    "#             in_length_tensor = np.full((1,int(input_batch_size_)),pred_mat.shape[0])\n",
    "#             in_length_tensor = torch.LongTensor(in_length_tensor)\n",
    "#             target_length_tensor = torch.LongTensor(target_lengths)\n",
    "#             #loss = F.nll_loss(output, target)\n",
    "#             #pred_mat = pred_mat.permute(1,0,2)\n",
    "#             #cc = ((encoded_target_test - pred).sum(axis=1) == 0).sum()\n",
    "#             #correct += cc\n",
    "#             encoded_target_test = encoded_target_test.to(device)\n",
    "#             in_length_tensor = in_length_tensor.to(device)\n",
    "#             target_length_tensor = target_length_tensor.to(device)\n",
    "#             loss = ctc_loss(pred_mat, encoded_target_test, in_length_tensor, target_length_tensor)\n",
    "            print('Inside testing')\n",
    "#---------------FOR CPU WITHOUT LEVENSHTEIN DISTSANCE-----------------------------------------\n",
    "            #distance = edit_dist_calc_list(targ_words,pred_words)\n",
    "            #writer.add_scalar('distance', distance, epoch)  ## To Display in TensorboardX\n",
    "            #print(distance)\n",
    "\n",
    "\n",
    "    #test_loss /= len(test_loader.dataset)\n",
    "    #print('\\nTest set: Average loss: {:.4f}'.format(Avg_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Collate_fn for Dataloader, this is used because the target_length for different length \n",
    "def my_collate_concat_label(batch_):\n",
    "    print('inside my_collate_concat_label')\n",
    "    print(type(batch_))\n",
    "    data = [item[0] for item in batch_]\n",
    "    #data = torch.LongTensor(data)\n",
    "    target = [item[1] for item in batch_]\n",
    "    lenn = tuple([item[2] for item in batch_])\n",
    "    target = np.concatenate(target)\n",
    "    #lenn = np.concatenate(lenn)\n",
    "    batch_tuple = (data,target,lenn)\n",
    "    return batch_tuple\n",
    "def absolute_to_relative_path(dataset_csv,dir_name):\n",
    "    path = 'path'\n",
    "    dataset_csv[path] = [dir_name+('/'.join(x.split('/')[-3:])) for x in dataset_csv[path]]\n",
    "    return dataset_csv\n",
    "def change_path(dataset_csv,root_dir_path):    \n",
    "    dataset_csv['path'] = root_dir_path + dataset_csv['relative_path']\n",
    "    df_all = dataset_csv\n",
    "    df_all = df_all.dropna()\n",
    "    df_all = df_all.reset_index(drop=True)\n",
    "    return df_all\n",
    "def warmup(model,full_dataset_as_csv,initial_word_width,W_warm_up_epochs,total_epochs,device):\n",
    "    l = initial_word_width\n",
    "    optimizer_w = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "    alpha_w = 0.00001\n",
    "    beta_w = 0.001\n",
    "    delta_w = (np.log10(beta_w)-np.log10(alpha_w))/total_epochs\n",
    "    #ln_alpha = np.log10(alpha)\n",
    "    #delta1 = (np.log10(beta-alpha))\n",
    "    #print(delta)\n",
    "    log_lr = np.log10(alpha_w)\n",
    "    for i in range(W_warm_up_epochs):\n",
    "\n",
    "        ln_lr = log_lr + i*delta_w\n",
    "        modified_lr = 10**ln_lr\n",
    "        for param_group in optimizer_w.param_groups:\n",
    "            param_group['lr'] = modified_lr\n",
    "        #Generate filtered training data based on length for warmup\n",
    "        #Filters and returns a Dataframe of the path, name, len as features\n",
    "        limited_leng = get_images_with_wordlength_less_than(full_dataset_as_csv,l)\n",
    "        \n",
    "        ## Map_chars dict is used to encode the chars to number and reverse_map to decode, set the width\n",
    "        dataset_filtered_length = images_random_text_dataset(limited_leng,map_chars_aadhar,reverse_map_aadhar,width_ = 128)\n",
    "        print('dataset_filtered_length :' + str(len(dataset_filtered_length)))\n",
    "        l+=1\n",
    "        dataset1 = dataset_filtered_length\n",
    "        train_length = int(len(dataset1)*0.8)\n",
    "        test_length = len(dataset1) - train_length\n",
    "        lengths = [train_length,test_length]\n",
    "        #print(lengths)\n",
    "        dataset_train,dataset_test = torch.utils.data.random_split(dataset1, lengths)\n",
    "        data_loader_train = torch.utils.data.DataLoader(dataset_train,batch_size=100,collate_fn=my_collate_concat_label)\n",
    "        #print(len(data_loader_train))\n",
    "        data_loader_test = torch.utils.data.DataLoader(dataset_test,batch_size=50,collate_fn=my_collate_concat_label)\n",
    "        train(device=device,epoch=i+1,model=model,train_loader=data_loader_train,optimizer=optimizer_w)\n",
    "        torch.save(model.state_dict(),weight_path)\n",
    "        test(device=device,model=model,test_loader=data_loader_test)\n",
    "    return model\n",
    "        \n",
    "def curriculum_train(model,full_dataset_as_csv,epoch_count,device,initial_width=30,initial_lr = 0.01,decay = 40):\n",
    "    wid = initial_width\n",
    "    modified_lr_c = initial_lr\n",
    "    #t=decay\n",
    "    optimizer_curr = torch.optim.Adam(model.parameters(), lr = modified_lr_c)\n",
    "    for i in range(epoch_count):\n",
    "        #learning_rate = learning_rate*10**(-1*(math.floor((i+3)/t)))\n",
    "        \n",
    "        for param_group in optimizer_curr.param_groups:\n",
    "            param_group['lr'] = modified_lr_c\n",
    "        modified_lr_c = modified_lr_c*10**(-1*(i/decay))\n",
    "        print(modified_lr_c)\n",
    "        #print(width)\n",
    "        limited_leng = get_images_with_image_width_less_than(full_dataset_as_csv,wid)\n",
    "        wid += 8\n",
    "        dataset_filtered_width = images_random_text_dataset(limited_leng,map_chars_aadhar,reverse_map_aadhar,width_ = wid)\n",
    "        dataset1 = dataset_filtered_width\n",
    "        \n",
    "        train_length = int(len(dataset1)*0.8)\n",
    "        test_length = len(dataset1) - train_length\n",
    "        lengths = [train_length,test_length]\n",
    "        dataset_train,dataset_test = torch.utils.data.random_split(dataset1, lengths)\n",
    "        data_loader_train = torch.utils.data.DataLoader(dataset_train,batch_size=100,collate_fn=my_collate_concat_label)\n",
    "        data_loader_test = torch.utils.data.DataLoader(dataset_test,batch_size=50,collate_fn=my_collate_concat_label)\n",
    "        train(device=device,epoch=i+1,model=model,train_loader=data_loader_train,optimizer=optimizer_curr)\n",
    "        torch.save(model.state_dict(),weight_path)\n",
    "        test(device=device,model=model,test_loader=data_loader_test)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_length_ = 2\n",
    "W_warm_up_epochs_ = 12\n",
    "curriculum_epochs_ = 40\n",
    "warm_up_width_ = 128\n",
    "initial_word_width_ = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001148153621496883\n",
      "0.00013182567385564074\n",
      "0.00015135612484362088\n",
      "0.00017378008287493763\n",
      "0.00019952623149688788\n",
      "0.00022908676527677723\n",
      "0.00026302679918953814\n",
      "0.0003019951720402016\n",
      "0.0003467368504525317\n",
      "0.00039810717055349735\n",
      "0.0004570881896148752\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,12):\n",
    "    ln_lr = ln_alpha + i*delta\n",
    "    print(10**ln_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "char_set_aadhar = string.ascii_lowercase + ' -/:' + string.digits\n",
    "\n",
    "map_chars_aadhar = {x:i+1 for i,x in enumerate(set(char_set_aadhar))}\n",
    "map_chars_aadhar['_'] = 0\n",
    "reverse_map_aadhar = {map_chars_aadhar[y]:y for y in map_chars_aadhar.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_set_aadhar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn1 type<class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "bn1 type<class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "bn1 type<class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "bn1 type<class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer= SummaryWriter('/home/madhevan/Documents/projects/rosetta/tensorboard_files/')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model1 = ResBlock(ResidualBlock,[1,1,1,1]).to(device)\n",
    "weight_path = '/home/madhevan/Documents/projects/rosetta/to_gcp/trial_gcp_rosetta_23.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_filtered_length :0\n",
      "dataset_filtered_length :24441\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([100, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([100, 30, 384])\n",
      "Linear layer output size :torch.Size([100, 30, 40])\n",
      "After last layer torch.Size([100, 30, 40])\n",
      "Train Epoch: 2 [0/19552 (0%)]\tLoss: 48.920475\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([100, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([100, 30, 384])\n",
      "Linear layer output size :torch.Size([100, 30, 40])\n",
      "After last layer torch.Size([100, 30, 40])\n",
      "Train Epoch: 2 [100/19552 (1%)]\tLoss: 47.080410\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([100, 3, 32, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-a8e1470d81c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata_with_full_path_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchange_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_details\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#model_ = warmup(model1,data_with_full_path_csv,initial_word_width_,W_warm_up_epochs_,42,device = 'cpu')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_with_full_path_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_word_width_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW_warm_up_epochs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mcurriculum_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_with_full_path_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-707d5accc1e6>\u001b[0m in \u001b[0;36mwarmup\u001b[0;34m(model, full_dataset_as_csv, initial_word_width, W_warm_up_epochs, total_epochs, device)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m#print(len(data_loader_train))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mdata_loader_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_collate_concat_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_loader_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-e3e59f5c1a3c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mimage_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mprob_sequence_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m#prob_sequence_matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprob_sequence_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob_sequence_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9dc80f2d875f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#         print('After 3rd res block layer ' + str(out.size()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;31m#         print('After 4th res block layer ' + str(out.size()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastConvlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-c30fbd45439f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 313\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "csv_path_name = '/home/madhevan/Documents/projects/rosetta/to_gcp/dataset_details_csv.csv'\n",
    "data_details = pd.read_csv(csv_path_name,index_col=0)\n",
    "#data_details = data_details[0:1000]\n",
    "#root_dir = '/'.join(csv_path_name.split('/')[0:(len(csv_path_name.split('/'))-1)])+'/'\n",
    "root_dir = '/home/madhevan/Documents/projects/rosetta/data_generation/'\n",
    "\n",
    "data_with_full_path_csv = change_path(data_details,root_dir)\n",
    "#model_ = warmup(model1,data_with_full_path_csv,initial_word_width_,W_warm_up_epochs_,42,device = 'cpu')\n",
    "model_ = warmup(model1,data_with_full_path_csv,initial_word_width_,W_warm_up_epochs_,42,device = 'cpu')\n",
    "curriculum_train(model1,data_with_full_path_csv,epoch_count=30,device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_filtered_length :0\n",
      "[0, 0]\n",
      "dataset_filtered_length :11\n",
      "[8, 3]\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([8, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([8, 30, 384])\n",
      "Linear layer output size :torch.Size([8, 30, 40])\n",
      "After last layer torch.Size([8, 30, 40])\n",
      "Train Epoch: 2 [0/8 (0%)]\tLoss: nan\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "[23 21 15 20 30 40]\n",
      "(2, 2, 2)\n",
      "After reshaping for linear layer to collapse : torch.Size([3, 30, 384])\n",
      "Linear layer output size :torch.Size([3, 30, 40])\n",
      "After last layer torch.Size([3, 30, 40])\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[], [], []]\n",
      "Inside testing\n",
      "dataset_filtered_length :27\n",
      "[21, 6]\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([21, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([21, 30, 384])\n",
      "Linear layer output size :torch.Size([21, 30, 40])\n",
      "After last layer torch.Size([21, 30, 40])\n",
      "Train Epoch: 3 [0/21 (0%)]\tLoss: nan\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "[40 32 35 30 12 40 32 30 40 40 20  8  1]\n",
      "(3, 2, 2, 2, 2, 2)\n",
      "After reshaping for linear layer to collapse : torch.Size([6, 30, 384])\n",
      "Linear layer output size :torch.Size([6, 30, 40])\n",
      "After last layer torch.Size([6, 30, 40])\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[], [], [], [], [], []]\n",
      "Inside testing\n",
      "dataset_filtered_length :63\n",
      "[50, 13]\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([50, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([50, 30, 384])\n",
      "Linear layer output size :torch.Size([50, 30, 40])\n",
      "After last layer torch.Size([50, 30, 40])\n",
      "Train Epoch: 4 [0/50 (0%)]\tLoss: nan\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "[23 30  5 21 23 30  5 21 12 30 34 23 32 35 21 17 12 30 15 30 40 15 30 23\n",
      "  5 23 30  5 21 13 24 35 21  1 23 30  5 21 23 30  5 21 23 21]\n",
      "(4, 4, 3, 4, 4, 2, 4, 4, 2, 3, 4, 4, 2)\n",
      "After reshaping for linear layer to collapse : torch.Size([13, 30, 384])\n",
      "Linear layer output size :torch.Size([13, 30, 40])\n",
      "After last layer torch.Size([13, 30, 40])\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Inside testing\n",
      "dataset_filtered_length :75\n",
      "[60, 15]\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([60, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([60, 30, 384])\n",
      "Linear layer output size :torch.Size([60, 30, 40])\n",
      "After last layer torch.Size([60, 30, 40])\n",
      "Train Epoch: 5 [0/60 (0%)]\tLoss: nan\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "[12 30 23 23 30  5 21 40 30 22 20 40 13 24 23 30  5 21 23 30  5 21 30  3\n",
      " 40 30 17 12 30 15 23 30  5 21 40 32 39 23 30 23 32 35 31 38 22 23 30  5\n",
      " 21 23 21 21 30 40]\n",
      "(3, 4, 5, 2, 4, 4, 4, 4, 4, 3, 5, 3, 4, 3, 2)\n",
      "After reshaping for linear layer to collapse : torch.Size([15, 30, 384])\n",
      "Linear layer output size :torch.Size([15, 30, 40])\n",
      "After last layer torch.Size([15, 30, 40])\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Inside testing\n",
      "dataset_filtered_length :121\n",
      "[96, 25]\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([96, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([96, 30, 384])\n",
      "Linear layer output size :torch.Size([96, 30, 40])\n",
      "After last layer torch.Size([96, 30, 40])\n",
      "Train Epoch: 6 [0/96 (0%)]\tLoss: nan\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "[40 32 39  3 21 23 30  5 21 17 12 30 35 40 23 30  5 21 15 21 40  1 30 34\n",
      "  3 21 23 30  5 21 23 30  5 21  3 21 23 30  5 21 40 38 35 23 30  5 21 35\n",
      " 30 12 34 20 40  1  3 21 23 30  5 21 23 30  5 21 35 20 40  1 30 30 35 15\n",
      " 20 22 15 30 13 30  5 34 23 30  5 21  3 21 23 30  5 21  3 21 23 30  5 21\n",
      " 23 30  5 21  3 21 23 30  5 21  8  1 21 22 30 35 23 38 35 20 15 30  3 21\n",
      " 23 30  5 21]\n",
      "(3, 6, 5, 4, 6, 6, 4, 6, 3, 4, 3, 4, 6, 4, 5, 5, 6, 4, 6, 6, 4, 6, 6, 6, 6)\n",
      "After reshaping for linear layer to collapse : torch.Size([25, 30, 384])\n",
      "Linear layer output size :torch.Size([25, 30, 40])\n",
      "After last layer torch.Size([25, 30, 40])\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Inside testing\n",
      "dataset_filtered_length :130\n",
      "[104, 26]\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([100, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([100, 30, 384])\n",
      "Linear layer output size :torch.Size([100, 30, 40])\n",
      "After last layer torch.Size([100, 30, 40])\n",
      "Train Epoch: 7 [0/104 (0%)]\tLoss: nan\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([4, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([4, 30, 384])\n",
      "Linear layer output size :torch.Size([4, 30, 40])\n",
      "After last layer torch.Size([4, 30, 40])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [4/104 (50%)]\tLoss: nan\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "[40  1 30 23 30  5 21 15 30 13 30  5 34 13 24 40 32 23  3 21 23 30  5 21\n",
      "  3 21 23 30  5 21 35 20 40  1 30 30  3 40 30 40 32 27  1 30  8  1 30 40\n",
      "  3 21 23 30  5 21  3 21 23 30  5 21  3 21 23 30  5 21  3 21 23 30  5 21\n",
      " 39 21 21 17 20 15 23 21 21 30 12  3 21 23 30  5 21 23 30  5 21 23 30 23\n",
      " 32 35 17 30 12 30 40  3 21 23 30  5 21 23 30  5 21]\n",
      "(3, 4, 6, 2, 3, 6, 6, 5, 4, 5, 2, 2, 6, 6, 6, 6, 6, 3, 2, 6, 4, 5, 3, 2, 6, 4)\n",
      "After reshaping for linear layer to collapse : torch.Size([26, 30, 384])\n",
      "Linear layer output size :torch.Size([26, 30, 40])\n",
      "After last layer torch.Size([26, 30, 40])\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Inside testing\n",
      "dataset_filtered_length :140\n",
      "[112, 28]\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([100, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([100, 30, 384])\n",
      "Linear layer output size :torch.Size([100, 30, 40])\n",
      "After last layer torch.Size([100, 30, 40])\n",
      "Train Epoch: 8 [0/112 (0%)]\tLoss: nan\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([12, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([12, 30, 384])\n",
      "Linear layer output size :torch.Size([12, 30, 40])\n",
      "After last layer torch.Size([12, 30, 40])\n",
      "Train Epoch: 8 [12/112 (50%)]\tLoss: nan\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "[12 30 23  3 21 23 30  5 21  3 21 23 30  5 21  3 21 23 30  5 21 35 20 40\n",
      "  1 30 23 30  5 21 34 20 35 38 13 30 31 39 20 40 38 35  3 21 23 30  5 21\n",
      " 13 24 23 30  5 21 27  1 32 17 21 35 39 13 24 38 22 20 23 30 23 32 35 12\n",
      " 21 40  1 23 30 23 40 30 27 30 35 30 40 32 23 17 20 35 35 20 12 23 30  5\n",
      " 30 39 17 12 30 35 40 30 12 15 30 13 30  5 34 17 12 30 34 21 21 12 30 34\n",
      "  3 21 23 30  5 21 40 32 23 20 22 23 30  5 21]\n",
      "(3, 6, 6, 6, 5, 4, 4, 5, 3, 6, 2, 4, 7, 5, 5, 7, 6, 3, 3, 8, 5, 2, 6, 6, 3, 6, 5, 4)\n",
      "After reshaping for linear layer to collapse : torch.Size([28, 30, 384])\n",
      "Linear layer output size :torch.Size([28, 30, 40])\n",
      "After last layer torch.Size([28, 30, 40])\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Inside testing\n",
      "dataset_filtered_length :153\n",
      "[122, 31]\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([100, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([100, 30, 384])\n",
      "Linear layer output size :torch.Size([100, 30, 40])\n",
      "After last layer torch.Size([100, 30, 40])\n",
      "Train Epoch: 9 [0/122 (0%)]\tLoss: nan\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([22, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([22, 30, 384])\n",
      "Linear layer output size :torch.Size([22, 30, 40])\n",
      "After last layer torch.Size([22, 30, 40])\n",
      "Train Epoch: 9 [22/122 (50%)]\tLoss: nan\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "[ 3 21 23 30  5 21 30 35 15 20 22  3 21 23 30  5 21 12 30 23 25 30 22 20\n",
      " 25  1  3 21 23 30  5 21 23 30  5 21 15 30 23  5 40 32 39 23 30  5 21  3\n",
      " 21 23 30  5 21 23 38 35 20 15 30 40 38 35 13 32  1 20  5 17 27 30 12 13\n",
      " 32 35 22 24  6 23 30  5 21 17 38 38 35 30 23 13 24  3 30 12 13 30 35 30\n",
      "  3 21 23 30  5 21 40 20 23 30  5 21  3 21 23 30  5 21  3 21 23 30  5 21\n",
      "  3 21 23 30  5 21  8  1 21 22 30 35 23 30  5 21  3 21 23 30  5 21 34 20\n",
      " 40  1  3 21 23 30  5 21 12 32  8  1 30 40]\n",
      "(6, 5, 6, 9, 6, 4, 4, 3, 4, 6, 6, 3, 7, 8, 4, 6, 2, 7, 6, 2, 4, 6, 6, 6, 6, 4, 6, 4, 6, 4, 2)\n",
      "After reshaping for linear layer to collapse : torch.Size([31, 30, 384])\n",
      "Linear layer output size :torch.Size([31, 30, 40])\n",
      "After last layer torch.Size([31, 30, 40])\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Inside testing\n",
      "dataset_filtered_length :164\n",
      "[131, 33]\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([100, 3, 32, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reshaping for linear layer to collapse : torch.Size([100, 30, 384])\n",
      "Linear layer output size :torch.Size([100, 30, 40])\n",
      "After last layer torch.Size([100, 30, 40])\n",
      "Train Epoch: 10 [0/131 (0%)]\tLoss: nan\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([31, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([31, 30, 384])\n",
      "Linear layer output size :torch.Size([31, 30, 40])\n",
      "After last layer torch.Size([31, 30, 40])\n",
      "Train Epoch: 10 [31/131 (50%)]\tLoss: nan\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "[30 40  1 20 23 30 23 23 30 35  1 21 23 30 35 22 15 23 30  5 21  3 21 23\n",
      " 30  5 21 17 38 38 35 30 23 17 12 30 35 40  3 21 23 30  5 21 13 24 38 22\n",
      " 20 22 30 40  5 20 23 30 30 15 23 20 40 23 30  5 21 23 30  5 21 30 12 13\n",
      " 32 35 22 24  6 23 32 35 21 40 20 17 30 12 17 12 30 34 21 21  3 21 23 30\n",
      "  5 21 12 30  8  1 40  1 30 23 30  5 21 23 30 23 32 35 30 12 40  1 23 30\n",
      "  5 21 30 40 12 30 23 15 20 30 40  1 30 23 30  5 21  3 21 23 30  5 21 30\n",
      "  3 40 30 30 40 40 30 27 30 35 30 40 32]\n",
      "(7, 3, 7, 4, 6, 6, 5, 6, 5, 10, 2, 4, 4, 8, 4, 2, 3, 6, 6, 4, 3, 4, 5, 4, 4, 2, 9, 4, 6, 4, 2, 6, 2)\n",
      "After reshaping for linear layer to collapse : torch.Size([33, 30, 384])\n",
      "Linear layer output size :torch.Size([33, 30, 40])\n",
      "After last layer torch.Size([33, 30, 40])\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Inside testing\n",
      "dataset_filtered_length :186\n",
      "[148, 38]\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([100, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([100, 30, 384])\n",
      "Linear layer output size :torch.Size([100, 30, 40])\n",
      "After last layer torch.Size([100, 30, 40])\n",
      "Train Epoch: 11 [0/148 (0%)]\tLoss: nan\n",
      "inside my_collate_concat_label\n",
      "<class 'list'>\n",
      "torch.Size([48, 3, 32, 128])\n",
      "After reshaping for linear layer to collapse : torch.Size([48, 30, 384])\n",
      "Linear layer output size :torch.Size([48, 30, 40])\n",
      "After last layer torch.Size([48, 30, 40])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-7488977c3ea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_with_full_path_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_word_width_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW_warm_up_epochs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcurriculum_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_with_full_path_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-df723c685d58>\u001b[0m in \u001b[0;36mwarmup\u001b[0;34m(model, full_dataset_as_csv, initial_word_width, W_warm_up_epochs, total_epochs, device)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m#print(len(data_loader_train))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mdata_loader_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_collate_concat_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_loader_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-5813fa527f95>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_sequence_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_target_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_length_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_length_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## To Display in TensorboardX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m#torch.nn.utils.clip_grad_value_(model.parameters(),1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aadhaar Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
